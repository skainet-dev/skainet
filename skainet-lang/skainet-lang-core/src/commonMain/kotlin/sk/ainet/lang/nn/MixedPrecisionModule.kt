package sk.ainet.lang.nn

import sk.ainet.lang.tensor.Tensor
import sk.ainet.lang.tensor.ops.MixedPrecisionTensorOps
import sk.ainet.lang.types.DType
//import sk.ainet.lang.nn.dsl.LayerConfig

/**
 * Abstract base class for modules that support mixed-precision operations.
 * This class handles automatic type conversions between different precision types,
 * enabling seamless integration of layers with different precision requirements.
 * 
 * The class manages the complexity of precision conversions, allowing derived
 * classes to focus on their core functionality while ensuring type safety
 * and performance optimization.
 * 
 * @param TInput The precision type for input tensors
 * @param TOutput The precision type for output tensors  
 * @param V The value type corresponding to the DType
 * @param inputType The input precision type instance
 * @param outputType The output precision type instance
 * @param conversionOps Mixed precision tensor operations for handling conversions
 * 
 * Example usage:
 * ```kotlin
 * class MixedPrecisionLinear<TInput : DType, TOutput : DType, V>(
 *     inputType: TInput,
 *     outputType: TOutput,
 *     conversionOps: MixedPrecisionTensorOps<V>,
 *     private val inFeatures: Int,
 *     private val outFeatures: Int
 * ) : MixedPrecisionModule<TInput, TOutput, V>(inputType, outputType, conversionOps) {
 *     
 *     override fun forwardImpl(input: Tensor<TInput, V>): Tensor<TOutput, V> {
 *         // Implement layer-specific logic here
 *         // Input and output conversions are handled automatically
 *     }
 * }
 * ```
 */
public abstract class MixedPrecisionModule<TInput : DType, TOutput : DType, V>(
    protected val inputType: TInput,
    protected val outputType: TOutput,
    protected val conversionOps: MixedPrecisionTensorOps<V>
) : Module<TInput, V>() {
    
    /**
     * Configuration for precision handling within this module.
     * Can be used to customize conversion behavior, caching, and validation.
     */
    //public var precisionConfig: LayerConfig<TInput, V>? = null
    
    /**
     * Whether to enable automatic input conversion.
     * When false, input tensors must already be in the expected input type.
     */
    public var autoConvertInput: Boolean = true
    
    /**
     * Whether to enable automatic output conversion.
     * When false, output tensors will be in the module's internal precision.
     */
    public var autoConvertOutput: Boolean = true
    
    /**
     * Cache for converted tensors to avoid repeated conversions.
     * Key is the tensor hash, value is the converted tensor.
     */
    private val conversionCache = mutableMapOf<Int, Tensor<*, V>>()
    
    /**
     * Statistics for monitoring conversion performance.
     */
    public val conversionStats: ConversionStats = ConversionStats()
    
    /**
     * Main forward pass implementation that handles precision conversions automatically.
     * This method wraps the actual implementation with conversion logic.
     */
    final override fun forward(input: Tensor<TInput, V>): Tensor<TInput, V> {
        conversionStats.totalForwardCalls++
        
        try {
            // Convert input if necessary and requested
            val convertedInput = if (autoConvertInput && !isSameType(input, inputType)) {
                conversionStats.inputConversions++
                convertInput(input)
            } else {
                input
            }
            
            // Call the actual implementation
            val internalOutput = forwardImpl(convertedInput)
            
            // Convert output if necessary and requested
            val finalOutput = if (autoConvertOutput && !isSameType(internalOutput, inputType)) {
                conversionStats.outputConversions++
                @Suppress("UNCHECKED_CAST")
                convertOutput(internalOutput) as Tensor<TInput, V>
            } else {
                @Suppress("UNCHECKED_CAST")
                internalOutput as Tensor<TInput, V>
            }
            
            return finalOutput
            
        } catch (e: Exception) {
            conversionStats.conversionErrors++
            throw MixedPrecisionException("Error in mixed precision forward pass: ${e.message}", e)
        }
    }
    
    /**
     * Abstract method that derived classes must implement.
     * This method contains the actual layer logic and operates on converted tensors.
     * 
     * @param input Input tensor in the expected input precision
     * @return Output tensor in the module's output precision
     */
    protected abstract fun forwardImpl(input: Tensor<TInput, V>): Tensor<TOutput, V>
    
    /**
     * Converts input tensor to the module's expected input precision.
     * Can be overridden by derived classes for custom conversion logic.
     */
    protected open fun convertInput(input: Tensor<TInput, V>): Tensor<TInput, V> {
        val cacheKey = input.hashCode()
        
        // Check cache first if caching is enabled
        /*
        if (precisionConfig?.cacheConvertedWeights == true) {
            val cached = conversionCache[cacheKey]
            if (cached != null) {
                conversionStats.cacheHits++
                @Suppress("UNCHECKED_CAST")
                return cached as Tensor<TInput, V>
            }
        }


         */
        // Perform conversion
        val converted = conversionOps.convert(input, inputType)
        
        // Cache result if enabled
       /* if (precisionConfig?.cacheConvertedWeights == true) {
            conversionCache[cacheKey] = converted
        }

        */
        
        return converted
    }
    
    /**
     * Converts output tensor from the module's internal precision to the expected output precision.
     * Can be overridden by derived classes for custom conversion logic.
     */
    protected open fun convertOutput(output: Tensor<TOutput, V>): Tensor<TOutput, V> {
        return conversionOps.convert(output, outputType)
    }
    
    /**
     * Validates precision compatibility for the module configuration.
     * Should be called during module initialization.
     */
    public fun validatePrecisionConfig() {
        //precisionConfig?.validate()
        
        // Additional validation specific to mixed precision modules
        if (inputType == outputType && (autoConvertInput || autoConvertOutput)) {
            throw IllegalArgumentException(
                "Input and output types are the same (${inputType::class.simpleName}), " +
                "but conversion is enabled. Consider disabling auto-conversion for better performance."
            )
        }
    }
    
    /**
     * Clears the conversion cache to free memory.
     * Should be called periodically in long-running applications.
     */
    public fun clearConversionCache() {
        conversionCache.clear()
        conversionStats.cacheClears++
    }
    
    /**
     * Gets the current cache size for monitoring purposes.
     */
    public fun getCacheSize(): Int = conversionCache.size
    
    /**
     * Helper method to check if two tensors have the same precision type.
     */
    private fun isSameType(tensor: Tensor<*, V>, targetType: DType): Boolean {
        // This is a simplified check - in practice, this would need to be more sophisticated
        return tensor.toString().contains(targetType::class.simpleName ?: "Unknown")
    }
}

/**
 * Specialized mixed precision module for cases where input and output types are the same
 * but internal computation uses a different precision.
 * 
 * This is commonly used for layers where weights are stored in low precision (e.g., INT8)
 * but computations are performed in higher precision (e.g., FP16) for accuracy.
 * 
 * @param TIO The input/output precision type
 * @param TInternal The internal computation precision type
 * @param V The value type corresponding to the DType
 */
public abstract class InternalMixedPrecisionModule<TIO : DType, TInternal : DType, V>(
    private val ioType: TIO,
    private val internalType: TInternal,
    private val conversionOps: MixedPrecisionTensorOps<V>
) : Module<TIO, V>() {
    
    /**
     * Forward implementation that ensures output is converted back to input type.
     */
    override fun forward(input: Tensor<TIO, V>): Tensor<TIO, V> {
        // Convert input to internal precision if needed
        val internalInput = if (ioType != internalType) {
            conversionOps.convert(input, internalType)
        } else {
            @Suppress("UNCHECKED_CAST")
            input as Tensor<TInternal, V>
        }
        
        // Perform internal computation
        val internalOutput = forwardImpl(internalInput)
        
        // Convert back to IO precision if needed
        return if (internalType != ioType) {
            conversionOps.convert(internalOutput, ioType)
        } else {
            @Suppress("UNCHECKED_CAST")
            internalOutput as Tensor<TIO, V>
        }
    }
    
    /**
     * Abstract method for internal computation in the internal precision.
     */
    protected abstract fun forwardImpl(input: Tensor<TInternal, V>): Tensor<TInternal, V>
}

/**
 * Statistics class for tracking mixed precision conversion performance.
 */
public data class ConversionStats(
    var totalForwardCalls: Long = 0,
    var inputConversions: Long = 0,
    var outputConversions: Long = 0,
    var conversionErrors: Long = 0,
    var cacheHits: Long = 0,
    var cacheClears: Long = 0
) {
    /**
     * Gets the cache hit ratio as a percentage.
     */
    public fun getCacheHitRatio(): Double {
        val totalCacheableOperations = inputConversions + outputConversions
        return if (totalCacheableOperations > 0) {
            (cacheHits.toDouble() / totalCacheableOperations) * 100.0
        } else {
            0.0
        }
    }
    
    /**
     * Gets the conversion ratio (conversions per forward call).
     */
    public fun getConversionRatio(): Double {
        return if (totalForwardCalls > 0) {
            (inputConversions + outputConversions).toDouble() / totalForwardCalls
        } else {
            0.0
        }
    }
    
    /**
     * Resets all statistics.
     */
    public fun reset() {
        totalForwardCalls = 0
        inputConversions = 0
        outputConversions = 0
        conversionErrors = 0
        cacheHits = 0
        cacheClears = 0
    }
}

/**
 * Exception thrown when mixed precision operations fail.
 */
public class MixedPrecisionException(
    message: String,
    cause: Throwable? = null
) : RuntimeException(message, cause)