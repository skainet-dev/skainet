= Basic Tensor Operations
:toc: left
:toclevels: 3
:sectanchors:
:sectlinks:

Learn the fundamentals of working with tensors in SKaiNET through practical examples.

== What are Tensors?

Tensors are multi-dimensional arrays that generalize scalars, vectors, and matrices:

* **Scalar** (0D tensor): A single number
* **Vector** (1D tensor): An array of numbers
* **Matrix** (2D tensor): A rectangular array of numbers
* **Higher-dimensional tensors**: 3D, 4D, and beyond

== Creating Tensors

=== From Arrays

The most common way to create tensors is from data arrays:

[source,kotlin]
----
// Scalar (0D tensor)
val scalar: TensorFP32 = CpuTensorFP32.fromArray(
    Shape(), 
    floatArrayOf(42.0f)
)

// Vector (1D tensor)
val vector: TensorFP32 = CpuTensorFP32.fromArray(
    Shape(4), 
    floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f)
)

// Matrix (2D tensor)
val matrix: TensorFP32 = CpuTensorFP32.fromArray(
    Shape(2, 3), 
    floatArrayOf(1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f)
)

// 3D tensor (e.g., RGB image: height × width × channels)
val image: TensorFP32 = CpuTensorFP32.fromArray(
    Shape(32, 32, 3), 
    FloatArray(32 * 32 * 3) { it.toFloat() }
)
----

=== Understanding Shapes

The `Shape` class defines tensor dimensions:

[source,kotlin]
----
val shape = Shape(2, 3, 4)  // 2×3×4 tensor
println("Dimensions: ${shape.dimensions}")  // [2, 3, 4]
println("Total elements: ${shape.size}")    // 24
println("Rank: ${shape.rank}")              // 3
----

== Accessing Tensor Elements

Use the indexing operator to access individual elements:

[source,kotlin]
----
val matrix = CpuTensorFP32.fromArray(
    Shape(3, 3), 
    floatArrayOf(1f, 2f, 3f, 4f, 5f, 6f, 7f, 8f, 9f)
)

// Access elements
println("Element at [0,0]: ${matrix[0, 0]}")  // 1.0
println("Element at [1,2]: ${matrix[1, 2]}")  // 6.0
println("Element at [2,1]: ${matrix[2, 1]}")  // 8.0
----

== Printing Tensors

SKaiNET provides convenient extension functions to print tensors in a readable format:

[source,kotlin]
----
// Scalar (represented as 1D tensor with single element)
val scalar = CpuTensorFP32.fromArray(Shape(1), floatArrayOf(42.5f))
println("Scalar: ${scalar.print()}")
// Output: Scalar: 42.5

// Vector
val vector = CpuTensorFP32.fromArray(Shape(4), floatArrayOf(1f, 2f, 3f, 4f))
println("Vector: ${vector.print()}")
// Output: Vector: [1.0, 2.0, 3.0, 4.0]

// Matrix
val matrix = CpuTensorFP32.fromArray(
    Shape(2, 3), 
    floatArrayOf(1f, 2f, 3f, 4f, 5f, 6f)
)
println("Matrix:\n${matrix.print()}")
// Output: Matrix:
// [
//   [1.0, 2.0, 3.0],
//   [4.0, 5.0, 6.0]
// ]

// You can also use specific printing functions
println("Vector using printVector(): ${vector.printVector()}")
println("Matrix using printMatrix():\n${matrix.printMatrix()}")
----

== Basic Arithmetic Operations

=== Element-wise Operations

Perform operations on corresponding elements of tensors:

[source,kotlin]
----
val backend = CpuBackend()

val a = CpuTensorFP32.fromArray(Shape(2, 2), floatArrayOf(1f, 2f, 3f, 4f))
val b = CpuTensorFP32.fromArray(Shape(2, 2), floatArrayOf(5f, 6f, 7f, 8f))

// Addition
val sum = with(backend) { a + b }
// Result: [6, 8, 10, 12]

// Subtraction  
val diff = with(backend) { a - b }
// Result: [-4, -4, -4, -4]

// Element-wise multiplication
val product = with(backend) { a * b }
// Result: [5, 12, 21, 32]

// Element-wise division
val quotient = with(backend) { a / b }
// Result: [0.2, 0.33, 0.43, 0.5]
----

=== Scalar Operations

Apply operations with single values:

[source,kotlin]
----
val tensor = CpuTensorFP32.fromArray(Shape(2, 2), floatArrayOf(1f, 2f, 3f, 4f))

val backend = CpuBackend()
with(backend) {
    val scaled = tensor * 2.0f        // [2, 4, 6, 8]
    val shifted = tensor + 10.0f      // [11, 12, 13, 14]
    val normalized = tensor / 4.0f    // [0.25, 0.5, 0.75, 1.0]
}
----

== Practical Example: Data Normalization

A common preprocessing step in machine learning:

[source,kotlin]
----
fun normalizeData(data: CpuTensorFP32, backend: CpuBackend): CpuTensorFP32 {
    // Find min and max values (simplified approach)
    var min = Float.MAX_VALUE
    var max = Float.MIN_VALUE
    
    val shape = data.shape
    for (i in 0 until shape.dimensions[0]) {
        for (j in 0 until shape.dimensions[1]) {
            val value = data[i, j]
            if (value < min) min = value
            if (value > max) max = value
        }
    }
    
    // Normalize to [0, 1] range
    val range = max - min
    return with(backend) {
        (data - min) / range
    }
}

// Example usage
val rawData = CpuTensorFP32.fromArray(
    Shape(3, 4), 
    floatArrayOf(10f, 20f, 30f, 40f, 50f, 60f, 70f, 80f, 90f, 100f, 110f, 120f)
)

val backend = CpuBackend()
val normalized = normalizeData(rawData, backend)

println("Original range: 10-120")
println("Normalized range: 0-1")
----

== Practical Example: Distance Calculations

Calculate Euclidean distance between data points:

[source,kotlin]
----
fun euclideanDistance(
    point1: CpuTensorFP32, 
    point2: CpuTensorFP32, 
    backend: CpuBackend
): Float {
    // Calculate difference
    val diff = with(backend) { point1 - point2 }
    
    // Square the differences
    val squared = with(backend) { diff * diff }
    
    // Sum all elements (manual implementation for demonstration)
    var sum = 0.0f
    for (i in 0 until squared.shape.dimensions[0]) {
        sum += squared[i]
    }
    
    return kotlin.math.sqrt(sum)
}

// Example: Calculate distance between two 3D points
val point1 = CpuTensorFP32.fromArray(Shape(3), floatArrayOf(1f, 2f, 3f))
val point2 = CpuTensorFP32.fromArray(Shape(3), floatArrayOf(4f, 5f, 6f))

val backend = CpuBackend()
val distance = euclideanDistance(point1, point2, backend)
println("Distance: $distance")  // Should be ~5.196
----

== Practical Example: Moving Average

Smooth time series data using a simple moving average:

[source,kotlin]
----
fun movingAverage(
    timeSeries: CpuTensorFP32, 
    windowSize: Int, 
    backend: CpuBackend
): CpuTensorFP32 {
    val length = timeSeries.shape.dimensions[0]
    val result = FloatArray(length - windowSize + 1)
    
    for (i in 0..length - windowSize) {
        var sum = 0.0f
        for (j in i until i + windowSize) {
            sum += timeSeries[j]
        }
        result[i - 0] = sum / windowSize
    }
    
    return CpuTensorFP32.fromArray(Shape(result.size), result)
}

// Example: Smooth stock price data
val prices = CpuTensorFP32.fromArray(
    Shape(10), 
    floatArrayOf(100f, 102f, 98f, 105f, 103f, 107f, 104f, 109f, 106f, 111f)
)

val backend = CpuBackend()
val smoothed = movingAverage(prices, 3, backend)

println("Original prices: ${(0 until 10).map { prices[it] }}")
println("3-day moving average: ${(0 until smoothed.shape.dimensions[0]).map { smoothed[it] }}")
----

== Working with Different Data Types

SKaiNET supports different numeric precisions:

[source,kotlin]
----
// 32-bit floating point (most common)
val fp32 = CpuTensorFP32.fromArray(Shape(2, 2), floatArrayOf(1f, 2f, 3f, 4f))

// You can also work with different precisions as needed
// The type system ensures type safety
----

== Common Patterns and Best Practices

=== Use Backend Context

Always use the backend for operations:

[source,kotlin]
----
val backend = CpuBackend()

// Good: Use backend context
with(backend) {
    val result = tensor1 + tensor2
}

// Also good: Explicit backend calls
val result = backend.matmul(tensor1, tensor2)
----

=== Shape Compatibility

Ensure tensor shapes are compatible for operations:

[source,kotlin]
----
val a = CpuTensorFP32.fromArray(Shape(2, 3), floatArrayOf(1f, 2f, 3f, 4f, 5f, 6f))
val b = CpuTensorFP32.fromArray(Shape(2, 3), floatArrayOf(7f, 8f, 9f, 10f, 11f, 12f))

// This works - same shapes
val sum = with(backend) { a + b }

// This won't work - incompatible shapes
// val c = CpuTensorFP32.fromArray(Shape(3, 2), floatArrayOf(...))
// val invalid = with(backend) { a + c }  // Error!
----

== Next Steps

Now that you understand basic tensor operations, let's explore more advanced operations in xref:matrix-operations.adoc[Matrix Operations].

[TIP]
====
Practice these examples by modifying the data and operations. Understanding how tensors behave with different shapes and operations is crucial for more advanced applications.
====